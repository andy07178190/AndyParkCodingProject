# Create an influence plot to check leverage and influence
influencePlot(mobility_lm)
# Plot Cook's distances using autoplot (default 'which = 4' gives the Cook's distance plot)
autoplot(mobility_lm, which = 4)
# Identify the most influential observation based on Cook's distance:
cooks_vals <- cooks.distance(mobility_lm)
# Plot Cook's distances using autoplot (default 'which = 4' gives the Cook's distance plot)
autoplot(mobility_lm, which = 4)
# Identify the most influential observation based on Cook's distance:
cooks_vals <- cooks.distance(mobility_lm)
most_influential <- which.max(cooks_vals)
# Plot Cook's distances using autoplot (default 'which = 4' gives the Cook's distance plot)
autoplot(mobility_lm, which = 4)
# Create an influence plot to check leverage and influence
influencePlot(mobility_lm)
# Generate diagnostic plots using autoplot
autoplot(mobility_lm)
# a loop to fit all polynomial regression and calculate LOORsq
for (i in degrees) {
poly_lm <- lm(accel ~ poly(times, degree=i), data=mcycle)
# calculate LOO prediction y_hat_{-i}
loo_poly_lm <- loo_predictions(poly_lm)
# LOORsq = cor(y_hat_{-i}, y)^2
LOORsq[i] <- cor(loo_poly_lm, mcycle$accel)^2
}
# make a plot to visualize
plot(x=degrees, y=LOORsq)
## Chapter 15 Logistic Regression
# Ex 15.10.2
default_file <- paste0("https://raw.githubusercontent.com/dsnair/", "ISLR/master/data/csv/Default.csv")
# Describe the model
# medv = beta0 + beta1*nox + beta2*dis + beta3*nox*dis + beta4*lstat
# medv = beta0 + + beta2*dis + (beta1 + beta3*dis)*nox + beta4*lstat
# medv = beta0 + (beta2 + beta3*nox)*dis + beta1*nox + beta4*lstat
# Given a research question, what is the appropriate model?
# 12.8.1(b)
lm2 <- lm(medv ~ nox * dis + nox * lstat, data=Boston)
summary(lm2)
# are needed for predicting medv?
## Based on the summary table, lstat and nox:dis are pretty significant, since both of them are lower than 0.05. However, since nox, dis, and nox:lstat are higher than 0.05, we cannot say all data are needed for prediciton.
# 12.8.1(d) Interpret the interaction between nox and dis in part (a),
# in terms of how changing nox causes the effect of dis on medv to change.
## As nox increase, the medv get decrease, and finally get to negative value. That means as nox increase, the positive effect of dis getting decrease, and finally reach to negative.
# 12.8.1(e) Interpret the interaction between nox and dis in part (a),
# in terms of how changing dis causes the effect of nox on medv to change.
## as distance increase, the effect of nox on medv getting decrease. But as the distance getting larger enought to reach medv to negative value, it affect negative,
# Ex 12.8.3
# run install.packages('ISLR') if you don't have the package
library(ISLR)
wage <- ISLR::Wage
?Wage
# 12.8.3(a)
lm3 <- lm(wage ~ age * education, data = wage)
summary(lm3)
## Chapter 13 Model Evaluation and Selection
# Ex 13.7.2
mcycle <- MASS::mcycle
?mcycle
# 13.7.2(a)
plot(x=mcycle$times, y=mcycle$accel, xlab='time in milliseconds after impact',
ylab='acceleration' )
# 13.7.2(c)
# we will fit polynomial regression with degree from 1 to 10 as illustration
# what is polynomial regression with degree 1?
fit1 <- lm(accel~ploy(time, 1)), data = mcycle)
# 13.7.2(c)
# we will fit polynomial regression with degree from 1 to 10 as illustration
# what is polynomial regression with degree 1?
# (1) 명시적으로 times 변수만 사용하는 선형 회귀
fit1 <- lm(accel ~ times, data = mcycle)
# (2) poly()를 써서 1차 다항이라고 지정
fit1 <- lm(accel ~ poly(times, 1), data = mcycle)
# 13.7.2(c)
# we will fit polynomial regression with degree from 1 to 10 as illustration
# what is polynomial regression with degree 1?
# (1) 명시적으로 times 변수만 사용하는 선형 회귀
fit1 <- lm(accel ~ times, data = mcycle)
# (2) poly()를 써서 1차 다항이라고 지정
fit1 <- lm(accel ~ poly(times, 1), data = mcycle)
fit
# 13.7.2(c)
# we will fit polynomial regression with degree from 1 to 10 as illustration
# what is polynomial regression with degree 1?
# (1) 명시적으로 times 변수만 사용하는 선형 회귀
fit1 <- lm(accel ~ times, data = mcycle)
fit1
# (2) poly()를 써서 1차 다항이라고 지정
fit1 <- lm(accel ~ poly(times, 1), data = mcycle)
# user-defined function to calculate LOO predictions for lm object
loo_predictions <- function(fit) {
e <- residuals(fit)
h <- hatvalues(fit)
Y <- fitted(fit) + e
return(Y - e / (1 - h))
}
fit1
fit1_raw <- lm(accel ~ poly(times, 1, raw=TRUE), data=mcycle)
fit1_raw <- lm(accel ~ poly(times, 1, raw=TRUE), data=mcycle)
summary(fit1_raw)
fit1_raw <- lm(accel ~ poly(times, 5, raw=TRUE), data=mcycle)
summary(fit1_raw)
# 13.7.2(c)
# we will fit polynomial regression with degree from 1 to 10 as illustration
# what is polynomial regression with degree 1?
# (1) 명시적으로 times 변수만 사용하는 선형 회귀
fit1 <- lm(accel ~ times, data = mcycle)
fit1
# (2) poly()를 써서 1차 다항이라고 지정
fit1 <- lm(accel ~ poly(times, 1), data = mcycle)
fit1
fit1_raw <- lm(accel ~ poly(times, 5, raw=TRUE), data=mcycle)
summary(fit1_raw)
# user-defined function to calculate LOO predictions for lm object
loo_predictions <- function(fit) {
e <- residuals(fit)
h <- hatvalues(fit)
Y <- fitted(fit) + e
return(Y - e / (1 - h))
}
# degrees to try
degrees = 1:10
# place holder to save LOORsq for different polynomial regression
LOORsq = rep(0, 10)
# a loop to fit all polynomial regression and calculate LOORsq
for (i in degrees) {
poly_lm <- lm(accel ~ poly(times, degree=i), data=mcycle)
# calculate LOO prediction y_hat_{-i}
loo_poly_lm <- loo_predictions(poly_lm)
# LOORsq = cor(y_hat_{-i}, y)^2
LOORsq[i] <- cor(loo_poly_lm, mcycle$accel)^2
}
# make a plot to visualize
plot(x=degrees, y=LOORsq)
# degrees to try
degrees = 1:10
# place holder to save LOORsq for different polynomial regression
LOORsq = rep(0, 10)
# a loop to fit all polynomial regression and calculate LOORsq
for (i in degrees) {
poly_lm <- lm(accel ~ poly(times, degree=i), data=mcycle)
# calculate LOO prediction y_hat_{-i}
loo_poly_lm <- loo_predictions(poly_lm)
# LOORsq = cor(y_hat_{-i}, y)^2
LOORsq[i] <- cor(loo_poly_lm, mcycle$accel)^2
}
# make a plot to visualize
plot(x=degrees, y=LOORsq)
# what is the best model based on LOORsq measure?
# what if we use other model selection criteria: AIC, BIC, AdjRsq, PRESS...
## Based on the LOORsq measure, the model with degree 9 is the best model based on LOORsq measure, since it has more biggest value for LOORsq among 1 to 10.
##
## Chapter 15 Logistic Regression
# Ex 15.10.2
default_file <- paste0("https://raw.githubusercontent.com/dsnair/", "ISLR/master/data/csv/Default.csv")
Default <- read.csv(default_file)
# take a look at the data
head(Default)
# recode the response to be 0 and 1
Default$default <- ifelse(Default$default=="Yes", 1, 0)
# fit the logistic regression model
# to predict default probability using student and balance
log1 <- glm(default ~ student + balance, data = Default, family = 'binomial')
summary(log1)
# how to make prediction???
# a student with balance of 2000
# calculate log odds (the linear predictor)
log_odds = -1.075e+01-7.149e-01+5.738e-03*2000
# calculate odds
odds = exp(log_odds)
# calculate probability
prob = odds / (1 + odds)
log_odds
prob
# use R predict function: the default is type="link" which returns log odds
predict(log1, newdata=data.frame(balance=2000, student="Yes"))
# use type="response" to return actual probability
predict(log1, newdata=data.frame(balance=2000, student="Yes"), type="response")
# how do we interpret confidence intervals for the coefficients???
confint(log1)
# fit the logistic regression model
# to predict default probability using student, balance and an interaction
log2 <- glm(default ~ student*balance, data = Default, family = 'binomial')
summary(log2)
# what is the log odds
log_odds = -1.087e+01-3.512e-01+(5.819e-03-2.196e-04)*2000
# what is the odds of default for a student with a balance of 2000?
odds = exp(log_odds)
prob = odds / (1+odds)
prob
# what is the prob of default for a non-student with a balance of 2000?
exp(-1.087e+01+5.819e-03*2000) / (exp(-1.087e+01+5.819e-03*2000)+1)
# 1) 데이터 불러오기
bp_file <- "https://raw.githubusercontent.com/theodds/SDS-348/master/bp.csv"
blood_pressure <- read.csv(bp_file)
# 2) 분석할 변수 이름 벡터로 정의
vars <- c("BP","Age","Weight","BSA","Dur","Pulse","Stress")
# 3) 상관 행렬 계산
bp_cor <- cor(blood_pressure[vars], use = "pairwise.complete.obs")
print(round(bp_cor, 3))   # 소수점 셋째 자리까지 보기
# 4) reshape2 패키지로 “길쭉한” 형태로 변환해서 보기
if (!requireNamespace("reshape2", quietly=TRUE)) install.packages("reshape2")
library(reshape2)
cor_df <- melt(bp_cor, varnames = c("Var1","Var2"), value.name = "r")
# 자기 자신과의 상관도(Var1==Var2)는 제외
cor_df <- subset(cor_df, Var1 != Var2)
# 5) 절댓값 기준으로 가장 큰 상관 5개
top_abs <- head(cor_df[order(-abs(cor_df$r)), ], 5)
print("가장 강한 (절댓값 기준) 상관 관계 5개:")
print(top_abs)
# 6) 양의 상관이 가장 큰 3개
top_pos <- head(cor_df[order(-cor_df$r), ], 3)
print("가장 큰 양의 상관 3개:")
print(top_pos)
# 7) 음의 상관이 가장 큰 3개
top_neg <- head(cor_df[order(cor_df$r), ], 3)
print("가장 큰 음의 상관 3개:")
print(top_neg)
# 8) 개별 쌍에 대해 p-value까지 보고 싶다면 cor.test() 사용
print("BP vs Age 상관 테스트:")
print(cor.test(blood_pressure$BP, blood_pressure$Age))
print("Weight vs BSA 상관 테스트:")
print(cor.test(blood_pressure$Weight, blood_pressure$BSA))
Boston <- MASS::Boston
head(Boston)
boston_sub <- select(Boston, -zn, -chas)
library(pheatmap)
bp_file <-
"https://raw.githubusercontent.com/theodds/SDS-348/master/bp.csv"
#--- 1) 패키지 로드 -----------------------------------------------------
library(dplyr)
library(pheatmap)
#--- 2) Boston 데이터 준비 ----------------------------------------------
library(MASS)
Boston <- MASS::Boston
# (기존에 select()를 쓰셨다면 dplyr은 이미 로드된 상태일 것)
boston_sub <- select(Boston, -zn, -chas)
boston_sub <- Boston %>%
select(-zn, -chas)
# 2) or, without dplyr, using base R:
boston_sub <- Boston[, !(names(Boston) %in% c("zn","chas"))]
boston_sub <- Boston %>%
select(-zn, -chas)
# 2) or, without dplyr, using base R:
boston_sub <- Boston[, !(names(Boston) %in% c("zn","chas"))]
# 1) load dplyr, then select()
library(dplyr)
boston_sub <- Boston %>%
select(-zn, -chas)
# 2) or, without dplyr, using base R:
boston_sub <- Boston[, !(names(Boston) %in% c("zn","chas"))]
#--- 3) boston_sub 상관행렬 계산 & 출력 --------------------------------
boston_cor <- cor(boston_sub, use = "pairwise.complete.obs")
print("===== boston_sub 상관행렬 (소수점 셋째자리 반올림) =====")
print(round(boston_cor, 3))
#--- 4) boston_sub 상관 히트맵 -------------------------------------------
pheatmap(boston_cor,
treeheight_col = 0,
treeheight_row = 0,
display_numbers = TRUE,
breaks = seq(-1, 1, length = 101),
main = "Boston Subset Correlation")
#--- 5) blood_pressure 데이터 준비 -------------------------------------
bp_file <- "https://raw.githubusercontent.com/theodds/SDS-348/master/bp.csv"
blood_pressure <- read.csv(bp_file)
#--- 6) blood_pressure 상관행렬 계산 & 출력 -----------------------------
bp_cor <- cor(blood_pressure[ , -1],  # Pt(첫 열)은 ID라 제외
use = "pairwise.complete.obs")
print("===== blood_pressure 상관행렬 (소수점 셋째자리 반올림) =====")
print(round(bp_cor, 3))
#--- 7) blood_pressure 상관 히트맵 --------------------------------------
pheatmap(bp_cor,
treeheight_col = 0,
treeheight_row = 0,
display_numbers = TRUE,
breaks = seq(-1, 1, length = 101),
main = "Blood Pressure Dataset Correlation")
#--- 8) (선택) 가장 강한 상관 뽑아보기 -----------------------------------
if (!requireNamespace("reshape2", quietly = TRUE)) install.packages("reshape2")
library(reshape2)
# melt 후 자기자신 상관(대각) 제거
cor_df_bp <- melt(bp_cor, varnames = c("Var1", "Var2"), value.name = "r")
cor_df_bp <- subset(cor_df_bp, Var1 != Var2)
# 절댓값 기준 Top5
top5_bp <- head(cor_df_bp[order(-abs(cor_df_bp$r)), ], 5)
print("===== blood_pressure 절댓값 기준 Top5 상관 =====")
print(top5_bp)
cor_df_boston <- melt(boston_cor, varnames = c("Var1", "Var2"), value.name = "r")
cor_df_boston <- subset(cor_df_boston, Var1 != Var2)
top5_boston <- head(cor_df_boston[order(-abs(cor_df_boston$r)), ], 5)
print("===== boston_sub 절댓값 기준 Top5 상관 =====")
print(top5_boston)
# 데이터 불러오기
filename    <- "https://raw.githubusercontent.com/theodds/SDS-383D/main/Challenger.csv"
challenger  <- read.csv(filename)
challenger  <- na.omit(challenger)
challenger$Fail <- ifelse(challenger$Fail=="yes", 1, 0)
# 로지스틱 회귀 적합
# logit[P(Fail=1)] = β0 + β1 * Temperature
model <- glm(Fail ~ Temperature, data=challenger, family=binomial)
summary(model)
glm(Fail ~ Temperature, family=binomial, data=challenger)
range(challenger$Temperature)
pred <- predict(challenger_glm,
newdata = data.frame(Temperature=31),
type="link",    # gives log-odds + se
se.fit=TRUE)
# Create visualization
plot(temps, probs,
type = 'l',
xlab = "Temperature (°F)",
ylab = "Estimated Probability of O-ring Failure",
main = "Estimated Probability of O-ring Failure vs Temperature")
# Add successes and failures
points(challenger$Temperature, challenger$Fail,
pch = 19,
cex = 1.2)
# Create sequence of temperatures for prediction
temps <- seq(from = 25, to = 85, by = 1)
# Get predicted probabilities
probs <- predict(challenger_glm,
newdata = data.frame(Temperature = temps),
type = "response")
# Create visualization
plot(temps, probs,
type = 'l',
xlab = "Temperature (°F)",
ylab = "Estimated Probability of O-ring Failure",
main = "Estimated Probability of O-ring Failure vs Temperature")
# Fit logistic regression
challenger_glm <- glm(Fail ~ Temperature,
data = challenger,
family = binomial)
# View the summary
summary(challenger_glm)
# Create response matrix for O-ring failures
Y_failures <- cbind(challenger$nFailures,
6 - challenger$nFailures)  # 6 minus failures = non-failures
# Fit model
failures_glm <- glm(Y_failures ~ Temperature,
family = binomial,
data = challenger)
summary(failures_glm)
# Fit logistic regression
challenger_glm <- glm(Fail ~ Temperature,
data = challenger,
family = binomial)
# View the summary
summary(challenger_glm)
# Create response matrix for O-ring failures
Y_failures <- cbind(challenger$nFailures,
6 - challenger$nFailures)  # 6 minus failures = non-failures
# Fit model
failures_glm <- glm(Y_failures ~ Temperature,
family = binomial,
data = challenger)
summary(failures_glm)
# Create sequence of temperatures for prediction
temps <- seq(from = 25, to = 85, by = 1)
# Get predicted probabilities
probs <- predict(challenger_glm,
newdata = data.frame(Temperature = temps),
type = "response")
# Create visualization
plot(temps, probs,
type = 'l',
xlab = "Temperature (°F)",
ylab = "Estimated Probability of O-ring Failure",
main = "Estimated Probability of O-ring Failure vs Temperature")
# Add successes and failures
points(challenger$Temperature, challenger$Fail,
pch = 19,
cex = 1.2)
predict(challenger_glm,
newdata = data.frame(Temperature = 31),
type = "response")
summary(challenger_glm)
summary(challenger_glm)
drop1(challenger_glm, test = "LRT")
## Fit the intercept-only challenger model
challenger_intercept <- glm(Fail ~ 1, data = challenger, family = binomial)
## Use anova() to compare intercept-only to the model that uses Temperature
anova(challenger_intercept, challenger_glm, test = "LRT")
drop1(challenger_glm, test = "LRT")
## Fit the intercept-only challenger model
challenger_intercept <- glm(Fail ~ 1, data = challenger, family = binomial)
## Use anova() to compare intercept-only to the model that uses Temperature
anova(challenger_intercept, challenger_glm, test = "LRT")
confint(challenger_glm, level = 0.95)
# Create sequence of temperatures for prediction
temps <- seq(from = 25, to = 85, by = 1)
# Get predicted probabilities
probs <- predict(challenger_glm,
newdata = data.frame(Temperature = temps),
type = "response")
# Get confidence intervals on link scale
preds_link <- predict(challenger_glm,
newdata = data.frame(Temperature = temps),
type = "link",
se.fit = TRUE)
# Convert to response scale
ci_lower <- plogis(preds_link$fit - 1.96 * preds_link$se.fit)
ci_upper <- plogis(preds_link$fit + 1.96 * preds_link$se.fit)
# Create visualization
plot(temps, probs, type = "l",
xlab = "Temperature (°F)",
ylab = "Probability of O-ring Failure",
main = "Probability of O-ring Failure vs Temperature")
lines(temps, ci_lower, col = 'forestgreen', lty = 2, lwd = 2)
lines(temps, ci_upper, col = 'forestgreen', lty = 2, lwd = 2)
# Add observed data points
points(challenger$Temperature, challenger$Fail,
pch = 19,
cex = 1.2)
# Get confidence intervals on link scale
preds_link <- predict(challenger_glm,
newdata = data.frame(Temperature = temps),
type = "link",
se.fit = TRUE)
# Convert to response scale using plogis() (the logistic function)
ci_lower <- plogis(preds_link$fit - 1.96 * preds_link$se.fit)
ci_upper <- plogis(preds_link$fit + 1.96 * preds_link$se.fit)
# fit the logistic regression model
# to predict default probability using student and balance
log1 <- glm(default ~ student + balance, data = Default, family = 'binomial')
# what is the best model based on LOORsq measure?
# what if we use other model selection criteria: AIC, BIC, AdjRsq, PRESS...
## Based on the LOORsq measure, the model with degree 9 is the best model based on LOORsq measure, since it has more biggest value for LOORsq among 1 to 10.
##
## Chapter 15 Logistic Regression
# Ex 15.10.2
default_file <- paste0("https://raw.githubusercontent.com/dsnair/", "ISLR/master/data/csv/Default.csv")
Default <- read.csv(default_file)
# what is the best model based on LOORsq measure?
# what if we use other model selection criteria: AIC, BIC, AdjRsq, PRESS...
## Based on the LOORsq measure, the model with degree 9 is the best model based on LOORsq measure, since it has more biggest value for LOORsq among 1 to 10.
##
## Chapter 15 Logistic Regression
# Ex 15.10.2
default_file <- paste0("https://raw.githubusercontent.com/dsnair/", "ISLR/master/data/csv/Default.csv")
Default <- read.csv(default_file)
# take a look at the data
head(Default)
# what is the best model based on LOORsq measure?
# what if we use other model selection criteria: AIC, BIC, AdjRsq, PRESS...
## Based on the LOORsq measure, the model with degree 9 is the best model based on LOORsq measure, since it has more biggest value for LOORsq among 1 to 10.
##
## Chapter 15 Logistic Regression
# Ex 15.10.2
default_file <- paste0("https://raw.githubusercontent.com/dsnair/", "ISLR/master/data/csv/Default.csv")
Default <- read.csv(default_file)
# take a look at the data
head(Default)
# recode the response to be 0 and 1
Default$default <- ifelse(Default$default=="Yes", 1, 0)
# fit the logistic regression model
# to predict default probability using student and balance
log1 <- glm(default ~ student + balance, data = Default, family = 'binomial')
summary(log1)
# how to make prediction???
# a student with balance of 2000
# calculate log odds (the linear predictor)
log_odds = -1.075e+01-7.149e-01+5.738e-03*2000
# calculate odds
odds = exp(log_odds)
# calculate probability
prob = odds / (1 + odds)
log_odds
prob
# calculate probability
prob = odds / (1 + odds)
log_odds
prob
# use R predict function: the default is type="link" which returns log odds
predict(log1, newdata=data.frame(balance=2000, student="Yes"))
# use type="response" to return actual probability
predict(log1, newdata=data.frame(balance=2000, student="Yes"), type="response")
# how do we interpret confidence intervals for the coefficients???
confint(log1)
# calculate odds
odds = exp(log_odds)
# calculate probability
prob = odds / (1 + odds)
log_odds
prob
# use R predict function: the default is type="link" which returns log odds
predict(log1, newdata=data.frame(balance=2000, student="Yes"))
# use type="response" to return actual probability
predict(log1, newdata=data.frame(balance=2000, student="Yes"), type="response")
# how do we interpret confidence intervals for the coefficients???
confint(log1)
t
# fit the logistic regression model
# to predict default probability using student, balance and an interaction
log2 <- glm(default ~ student*balance, data = Default, family = 'binomial')
summary(log2)
# fit the logistic regression model
# to predict default probability using student, balance and an interaction
log2 <- glm(default ~ student*balance, data = Default, family = 'binomial')
summary(log2)
# what is the log odds
log_odds = -1.087e+01-3.512e-01+(5.819e-03-2.196e-04)*2000
# what is the odds of default for a student with a balance of 2000?
odds = exp(log_odds)
prob = odds / (1+odds)
prob
# what is the prob of default for a non-student with a balance of 2000?
exp(-1.087e+01+5.819e-03*2000) / (exp(-1.087e+01+5.819e-03*2000)+1)
summary(lm1)
summary(lm1)
# 12.8.1(a)
lm1 <- lm(medv ~ nox * dis + lstat, data=Boston)
summary(lm1)
