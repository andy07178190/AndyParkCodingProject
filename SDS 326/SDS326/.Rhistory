rmsePoly  <- sqrt(mean(residPoly^2))
adjR2Poly <- summary(polyModel)$adj.r.squared
aicPoly   <- AIC(polyModel)
rmsePoly
adjR2Poly
aicPoly
# Secondary modeling to capture curvature
polyModel <- lm( quality ~ alcohol + I(alcohol^2) +
volatile_acidity +
sulphates +
residual_sugar, data = white)
summary(polyModel)
residPoly <- resid(polyModel)
rmsePoly  <- sqrt(mean(residPoly^2))
adjR2Poly <- summary(polyModel)$adj.r.squared
aicPoly   <- AIC(polyModel)
rmsePoly
adjR2Poly
aicPoly
# Secondary modeling to capture curvature
polyModel <- lm( quality ~ alcohol + I(alcohol^2) +
volatile_acidity +
sulphates +
residual_sugar, data = white)
summary(polyModel)
residPoly <- resid(polyModel)
rmsePoly  <- sqrt(mean(residPoly^2))
adjR2Poly <- summary(polyModel)$adj.r.squared
aicPoly   <- AIC(polyModel)
rmsePoly
adjR2Poly
aicPoly
# Secondary modeling to capture curvature
polyModel <- lm(
quality ~ alcohol + I(alcohol^2) +
volatile_acidity +
sulphates +
residual_sugar,
data = white
)
summary(polyModel)
rsePoly   <- summary(polyModel)$sigma
residPoly <- resid(polyModel)
rmsePoly  <- sqrt(mean(residPoly^2))
adjR2Poly <- summary(polyModel)$adj.r.squared
aicPoly   <- AIC(polyModel)
rsePoly
residPoly
rmsePoly
adjR2Poly
aicPoly
polyModel <- lm(
quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white)
summary(polyModel)
polyMetrics <- c(
RSE = summary(polyModel)$sigma,
RMSE = sqrt(mean(resid(polyModel)^2)),
AdjR2 = summary(polyModel)$adj.r.squared,
AIC = AIC(polyModel)
)
print(round(polyMetrics, 4))
# OLS Assumptions
library(dplyr)
library(tidyverse)
redWines <- read_csv("winequality_red.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "Red")
whiteWines <- read_csv("winequality_white.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "White")
wines <- bind_rows(whiteWines, redWines) %>%
mutate(
type  = factor(type, levels = c("White", "Red")),
isRed = as.integer(type == "Red")
)
white <- filter(wines, type == "White")
modelDiag <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white)
oldMfrow <- par("mfrow"); oldMar <- par("mar"); oldOma <- par("oma")
par(
mfrow = c(2, 2),
mar   = c(4.5, 4.5, 2.5, 1.5),
oma   = c(0,   0,   1.5, 0))
plot(modelDiag)
par(mfrow = oldMfrow, mar = oldMar, oma = oldOma)
# Drop the top 5 influences and refit
rseBase    <- summary(modelDiag)$sigma
adjR2Base  <- summary(modelDiag)$adj.r.squared
inflPoints  <- order(cooks.distance(modelDiag), decreasing = TRUE)[1:5]
trimmedData <- white[-inflPoints, ]
refitModel  <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = trimmedData)
rseRefit    <- summary(refitModel)$sigma
adjR2Refit  <- summary(refitModel)$adj.r.squared
deltaRSE    <- rseRefit - rseBase
deltaAdjR2  <- adjR2Refit - adjR2Base
print(round(rseBase, 4))
print(round(rseRefit, 4))
print(round(deltaRSE, 4))
print(round(adjR2Base, 4))
print(round(adjR2Refit, 4))
print(round(deltaAdjR2, 4))
baseCoefs <- summary(modelDiag)$coefficients
print(round(baseCoefs, 4))
refitCoefs <- summary(refitModel)$coefficients
print(round(refitCoefs, 4))
coefDelta <- round(refitCoefs[, "Estimate"] - baseCoefs[, "Estimate"], 4)
print(coefDelta)
# Exploring Nonlinearity in the Alcohol–Quality Relationship
library(tidyverse)
library(mgcv)
gamModel <- mgcv::gam(
quality ~ s(alcohol) + volatile_acidity + sulphates + residual_sugar,
data = white)
summary(gamModel)
oldMar <- par("mar")
par(mar = c(3, 3, 2, 1))
mgcv::plot.gam(gamModel, select = 1, rug = T, shade = T, se = T)
par(mar = oldMar)
# GAM AIC
aicGAM <- AIC(gamModel)
print(aicGAM, 2)
polyModel <- lm(
quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white)
summary(polyModel)
polyMetrics <- c(
RSE = summary(polyModel)$sigma,
RMSE = sqrt(mean(resid(polyModel)^2)),
AdjR2 = summary(polyModel)$adj.r.squared,
AIC = AIC(polyModel)
)
print(round(polyMetrics, 4))
# OLS Assumptions
library(dplyr)
library(tidyverse)
redWines <- read_csv("winequality_red.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "Red")
whiteWines <- read_csv("winequality_white.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "White")
wines <- bind_rows(whiteWines, redWines) %>%
mutate(
type  = factor(type, levels = c("White", "Red")),
isRed = as.integer(type == "Red")
)
white <- filter(wines, type == "White")
modelDiag <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white)
oldMfrow <- par("mfrow"); oldMar <- par("mar"); oldOma <- par("oma")
par(
mfrow = c(2, 2),
mar   = c(4.5, 4.5, 2.5, 1.5),
oma   = c(0,   0,   1.5, 0))
plot(modelDiag)
par(mfrow = oldMfrow, mar = oldMar, oma = oldOma)
# Drop the top 5 influences and refit
rseBase    <- summary(modelDiag)$sigma
adjR2Base  <- summary(modelDiag)$adj.r.squared
inflPoints  <- order(cooks.distance(modelDiag), decreasing = TRUE)[1:5]
trimmedData <- white[-inflPoints, ]
refitModel  <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = trimmedData)
rseRefit    <- summary(refitModel)$sigma
adjR2Refit  <- summary(refitModel)$adj.r.squared
deltaRSE    <- rseRefit - rseBase
deltaAdjR2  <- adjR2Refit - adjR2Base
print(round(rseBase, 4))
print(round(rseRefit, 4))
print(round(deltaRSE, 4))
print(round(adjR2Base, 4))
print(round(adjR2Refit, 4))
print(round(deltaAdjR2, 4))
baseCoefs <- summary(modelDiag)$coefficients
print(round(baseCoefs, 4))
refitCoefs <- summary(refitModel)$coefficients
print(round(refitCoefs, 4))
coefDelta <- round(refitCoefs[, "Estimate"] - baseCoefs[, "Estimate"], 4)
print(coefDelta)
aicBase <- AIC(modelDiag)
print(round(aicBase, 1))
polyModel <- lm(
quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white)
summary(polyModel)
polyMetrics <- c(
RSE = summary(polyModel)$sigma,
RMSE = sqrt(mean(resid(polyModel)^2)),
AdjR2 = summary(polyModel)$adj.r.squared,
AIC = AIC(polyModel)
)
print(round(polyMetrics, 4))
# Model Generalization 10-cross Performance Across Resamples
library(caret)
library(dplyr)
cvCtrl<- trainControl(method = "cv", number = 10)
cvOls <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvPoly <- train(quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvGam <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "gamLoess", trControl = cvCtrl)
resamps <- resamples(list(OLS  = cvOls, Poly = cvPoly, GAM  = cvGam))
summary(resamps)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
# OLS Assumptions
library(dplyr)
library(tidyverse)
redWines <- read_csv("winequality_red.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "Red")
whiteWines <- read_csv("winequality_white.csv") %>%
rename_with(~ gsub("[ .]", "_", .)) %>%
mutate(type = "White")
wines <- bind_rows(whiteWines, redWines) %>%
mutate(
type  = factor(type, levels = c("White", "Red")),
isRed = as.integer(type == "Red")
)
white <- filter(wines, type == "White")
modelDiag <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white)
oldMfrow <- par("mfrow"); oldMar <- par("mar"); oldOma <- par("oma")
par(
mfrow = c(2, 2),
mar   = c(4.5, 4.5, 2.5, 1.5),
oma   = c(0,   0,   1.5, 0))
plot(modelDiag)
par(mfrow = oldMfrow, mar = oldMar, oma = oldOma)
# Drop the top 5 influences and refit
rseBase    <- summary(modelDiag)$sigma
adjR2Base  <- summary(modelDiag)$adj.r.squared
inflPoints  <- order(cooks.distance(modelDiag), decreasing = TRUE)[1:5]
trimmedData <- white[-inflPoints, ]
refitModel  <- lm(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = trimmedData)
rseRefit    <- summary(refitModel)$sigma
adjR2Refit  <- summary(refitModel)$adj.r.squared
deltaRSE    <- rseRefit - rseBase
deltaAdjR2  <- adjR2Refit - adjR2Base
print(round(rseBase, 4))
print(round(rseRefit, 4))
print(round(deltaRSE, 4))
print(round(adjR2Base, 4))
print(round(adjR2Refit, 4))
print(round(deltaAdjR2, 4))
baseCoefs <- summary(modelDiag)$coefficients
print(round(baseCoefs, 4))
refitCoefs <- summary(refitModel)$coefficients
print(round(refitCoefs, 4))
coefDelta <- round(refitCoefs[, "Estimate"] - baseCoefs[, "Estimate"], 4)
print(coefDelta)
# Exploring Nonlinearity in the Alcohol–Quality Relationship
library(tidyverse)
library(mgcv)
gamModel <- mgcv::gam(
quality ~ s(alcohol) + volatile_acidity + sulphates + residual_sugar,
data = white)
summary(gamModel)
oldMar <- par("mar")
par(mar = c(3, 3, 2, 1))
mgcv::plot.gam(gamModel, select = 1, rug = T, shade = T, se = T)
par(mar = oldMar)
# GAM AIC
aicGAM <- AIC(gamModel)
print(aicGAM, 2)
# Secondary modeling to capture curvature
aicBase <- AIC(modelDiag)
print(round(aicBase, 1))
polyModel <- lm(
quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white)
summary(polyModel)
polyMetrics <- c(
RSE = summary(polyModel)$sigma,
RMSE = sqrt(mean(resid(polyModel)^2)),
AdjR2 = summary(polyModel)$adj.r.squared,
AIC = AIC(polyModel)
)
print(round(polyMetrics, 4))
# Model Generalization 10-cross Performance Across Resamples
library(caret)
library(dplyr)
cvCtrl<- trainControl(method = "cv", number = 10)
cvOls <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvPoly <- train(quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvGam <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "gamLoess", trControl = cvCtrl)
resamps <- resamples(list(OLS  = cvOls, Poly = cvPoly, GAM  = cvGam))
summary(resamps)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
# Model Generalization 10-cross Performance Across Resamples
library(caret)
library(dplyr)
cvCtrl<- trainControl(method = "cv", number = 10)
cvOls <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvPoly <- train(quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvGam <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "gamLoess", trControl = cvCtrl)
resamps <- resamples(list(OLS  = cvOls, Poly = cvPoly, GAM  = cvGam))
summary(resamps)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
summary(resamps)
# Model Generalization 10-cross Performance Across Resamples
library(caret)
library(dplyr)
cvCtrl<- trainControl(method = "cv", number = 10)
cvOls <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvPoly <- train(quality ~ alcohol + I(alcohol^2) + volatile_acidity + sulphates + residual_sugar, data = white, method = "lm", trControl = cvCtrl)
cvGam <- train(quality ~ alcohol + volatile_acidity + sulphates + residual_sugar, data = white, method = "gamLoess", trControl = cvCtrl)
resamps <- resamples(list(OLS  = cvOls, Poly = cvPoly, GAM  = cvGam))
summary(resamps)
# Boost Interpretation
library(xgboost)
library(caret)
library(dplyr)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
train_mat <- as.matrix(train %>% dplyr::select(-lbl))
train_lab <- as.numeric(train$lbl == "High")
test_mat <- as.matrix(test  %>% dplyr::select(-lbl))
test_lab <- as.numeric(test$lbl == "High")
dtrain <- xgb.DMatrix(data = train_mat, label = train_lab)
dtest <- xgb.DMatrix(data = test_mat, label = test_lab)
params <- list(objective = "binary:logistic", eval_metric = "auc")
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,
watchlist = list(train = dtrain, val = dtest),
early_stopping_rounds = 10,
verbose = 0
)
xgb_scores <- predict(xgb_model, dtest)
xgb_pred <- factor(ifelse(xgb_scores > 0.5, "High", "Low"),
levels = c("Low","High"))
print(caret::confusionMatrix(xgb_pred, test$lbl))
# Random Forest Classification
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
# Random Forest Classification
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
# Boost Interpretation
library(xgboost)
library(caret)
library(dplyr)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
train_mat <- as.matrix(train %>% dplyr::select(-lbl))
train_lab <- as.numeric(train$lbl == "High")
test_mat <- as.matrix(test  %>% dplyr::select(-lbl))
test_lab <- as.numeric(test$lbl == "High")
dtrain <- xgb.DMatrix(data = train_mat, label = train_lab)
dtest <- xgb.DMatrix(data = test_mat, label = test_lab)
params <- list(objective = "binary:logistic", eval_metric = "auc")
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,
watchlist = list(train = dtrain, val = dtest),
early_stopping_rounds = 10,
verbose = 0
)
xgb_scores <- predict(xgb_model, dtest)
xgb_pred <- factor(ifelse(xgb_scores > 0.5, "High", "Low"),
levels = c("Low","High"))
print(caret::confusionMatrix(xgb_pred, test$lbl))
# Random Forest Classification
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
# Random Forest Classification
library(randomForest)
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
rfpred <- predict(rfmodel, test)
print(caret::confusionMatrix(rfpred, test$lbl))
# ROC comparison
rocrf <- pROC::roc(test$lbl, predict(rfmodel, test, type = "prob")[,2])
rocxgb <- pROC::roc(test$lbl, xgb_scores)
plot(rocrf,  col = "blue", main = "ROC Curves")
lines(rocxgb, col = "red")
legend("bottomright", legend = c(paste("RF AUC =",  round(pROC::auc(rocrf), 3)),
paste("XGB AUC =", round(pROC::auc(rocxgb),3))),
col = c("blue","red"), lwd = 2)
# Assuming you already have rfmodel and xgb_scores from earlier:
library(pROC)
# 1. Compute ROC objects
rocrf  <- roc(test$lbl, predict(rfmodel, test, type="prob")[,2])
rocxgb <- roc(test$lbl, xgb_scores)
# 2. Plot RF ROC with diagonal “no‐skill” line and flipped x‐axis
plot(
rocrf,
col         = "blue",
legacy.axes = TRUE,               # flips x‐axis to 1−Specificity
main        = "ROC Curves",
xlab        = "1 - Specificity",
ylab        = "Sensitivity"
)
abline(a = 0, b = 1, lty = 2, col = "gray")  # dashed diagonal
# 3. Add XGBoost curve
lines(rocxgb, col = "red")
# 4. Shade 95% CI for each curve
ci_rf  <- ci.se(rocrf,  specificities = seq(0, 1, by = 0.1))
ci_xgb <- ci.se(rocxgb, specificities = seq(0, 1, by = 0.1))
plot(ci_rf,  type = "shape", col = adjustcolor("blue", 0.2), add = TRUE)
plot(ci_xgb, type = "shape", col = adjustcolor("red",  0.2), add = TRUE)
# 5. Legend with formatted AUCs
legend(
"bottomright",
legend = c(
sprintf("Random Forest (AUC = %.3f)", auc(rocrf)),
sprintf("XGBoost       (AUC = %.3f)", auc(rocxgb))
),
col = c("blue", "red"),
lwd = 2,
bty = "n"
)
# 6. Annotate “best” threshold points
opt_rf  <- coords(rocrf,  "best", ret = c("threshold","specificity","sensitivity"))
opt_xgb <- coords(rocxgb, "best", ret = c("threshold","specificity","sensitivity"))
# RF point
points(
1 - opt_rf["specificity"],
opt_rf["sensitivity"],
pch = 19, col = "blue"
)
text(
1 - opt_rf["specificity"],
opt_rf["sensitivity"] + 0.05,
paste0("RF thr=", round(opt_rf["threshold"], 2)),
col = "blue"
)
# XGB point
points(
1 - opt_xgb["specificity"],
opt_xgb["sensitivity"],
pch = 19, col = "red"
)
text(
1 - opt_xgb["specificity"],
opt_xgb["sensitivity"] - 0.05,
paste0("XGB thr=", round(opt_xgb["threshold"], 2)),
col = "red"
)
# Random Forest Classification
library(randomForest)
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
rfpred <- predict(rfmodel, test)
print(caret::confusionMatrix(rfpred, test$lbl))
# ROC comparison
rocrf <- pROC::roc(test$lbl, predict(rfmodel, test, type = "prob")[,2])
rocxgb <- pROC::roc(test$lbl, xgb_scores)
plot(rocrf,  col = "blue", main = "ROC Curves")
lines(rocxgb, col = "red")
legend("bottomright", legend = c(paste("RF AUC =",  round(pROC::auc(rocrf), 3)),
paste("XGB AUC =", round(pROC::auc(rocxgb),3))),
col = c("blue","red"), lwd = 2)
# Boost Interpretation
library(xgboost)
library(caret)
library(dplyr)
clf <- wines %>%
dplyr::mutate(lbl = factor(ifelse(quality >= 6, "High", "Low"), levels = c("Low","High"))) %>%
dplyr::select(alcohol, volatile_acidity, sulphates, residual_sugar, lbl)
set.seed(5)
idx <- caret::createDataPartition(clf$lbl, p = 0.8, list = F)
train <- clf[idx, ]
test <- clf[-idx, ]
train_mat <- as.matrix(train %>% dplyr::select(-lbl))
train_lab <- as.numeric(train$lbl == "High")
test_mat <- as.matrix(test  %>% dplyr::select(-lbl))
test_lab <- as.numeric(test$lbl == "High")
dtrain <- xgb.DMatrix(data = train_mat, label = train_lab)
dtest <- xgb.DMatrix(data = test_mat, label = test_lab)
params <- list(objective = "binary:logistic", eval_metric = "auc")
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,
watchlist = list(train = dtrain, val = dtest),
early_stopping_rounds = 10,
verbose = 0
)
xgb_scores <- predict(xgb_model, dtest)
xgb_pred <- factor(ifelse(xgb_scores > 0.5, "High", "Low"),
levels = c("Low","High"))
print(caret::confusionMatrix(xgb_pred, test$lbl))
# Random Forest Classification
library(randomForest)
rfmodel <- randomForest(lbl ~ ., data = train, ntree = 500)
rfpred <- predict(rfmodel, test)
print(caret::confusionMatrix(rfpred, test$lbl))
# ROC comparison
rocrf <- pROC::roc(test$lbl, predict(rfmodel, test, type = "prob")[,2])
rocxgb <- pROC::roc(test$lbl, xgb_scores)
plot(rocrf,  col = "blue", main = "ROC Curves")
lines(rocxgb, col = "red")
legend("bottomright", legend = c(paste("RF AUC =",  round(pROC::auc(rocrf), 3)),
paste("XGB AUC =", round(pROC::auc(rocxgb),3))),
col = c("blue","red"), lwd = 2)
